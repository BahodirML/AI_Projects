{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BahodirML/Coding_Practices/blob/main/YOLOv3_in_OpenCV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python==4.2.0.34\n",
        "!pip install opencv-python-headless==4.2.0.34\n"
      ],
      "metadata": {
        "id": "hZtI-HMCd6Ej",
        "outputId": "ee724106-a444-4b5b-a187-acdd8469d7a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement opencv-python==4.2.0.34 (from versions: 3.4.0.14, 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.15.55, 3.4.16.57, 3.4.16.59, 3.4.17.61, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64, 4.6.0.66, 4.7.0.68, 4.7.0.72, 4.8.0.74, 4.8.0.76, 4.8.1.78, 4.9.0.80)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for opencv-python==4.2.0.34\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement opencv-python-headless==4.2.0.34 (from versions: 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.15.55, 3.4.16.59, 3.4.17.61, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64, 4.6.0.66, 4.7.0.68, 4.7.0.72, 4.8.0.74, 4.8.0.76, 4.8.1.78, 4.9.0.80)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for opencv-python-headless==4.2.0.34\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BlZva9GdJVQ"
      },
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Define our imshow function\n",
        "def imshow(title = \"Image\", image = None, size = 10):\n",
        "    w, h = image.shape[0], image.shape[1]\n",
        "    aspect_ratio = w/h\n",
        "    plt.figure(figsize=(size * aspect_ratio,size))\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# # Download and unzip our images and YOLO files\n",
        "# !wget https://moderncomputervision.s3.eu-west-2.amazonaws.com/YOLO.zip\n",
        "# !unzip -qq YOLO.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URLs for the YOLOv3 config and weights files\n",
        "cfg_url = 'https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3.cfg'\n",
        "weights_url = 'https://pjreddie.com/media/files/yolov3.weights'\n",
        "names_url = 'https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names'\n",
        "\n",
        "# Download yolov3.cfg\n",
        "response = requests.get(cfg_url)\n",
        "with open('yolov3.cfg', 'wb') as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "# Download yolov3.weights\n",
        "response = requests.get(weights_url)\n",
        "with open('yolov3.weights', 'wb') as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "# Download coco.names\n",
        "response = requests.get(names_url)\n",
        "with open('coco.names', 'wb') as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "# Verify the downloads\n",
        "import os\n",
        "print(os.listdir('.'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EfuCcmkabCW",
        "outputId": "f97cab90-9ec0-44f1-8e82-9e03cba06052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'yolov3.cfg', 'yolov3.weights', 'coco.names', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb-FtCJRdOG8",
        "outputId": "7edb9343-6fae-4b72-c7a9-36a49f15b580"
      },
      "source": [
        "# Load the COCO class labels our YOLO model was trained on\n",
        "labelsPath = \"coco.names\"\n",
        "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
        "\n",
        "# We now need to initialize a list of colors to represent each possible class label\n",
        "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype=\"uint8\")\n",
        "\n",
        "print(\"Loading YOLO weights...\")\n",
        "\n",
        "weights_path = \"yolov3.weights\"\n",
        "cfg_path = \"yolov3.cfg\"\n",
        "\n",
        "# Create our blob object\n",
        "net = cv2.dnn.readNetFromDarknet(cfg_path, weights_path)\n",
        "\n",
        "# Set our backend\n",
        "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
        "# net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
        "\n",
        "print(\"Our YOLO Layers\")\n",
        "ln = net.getLayerNames()\n",
        "\n",
        "# There are 254 Layers\n",
        "print(len(ln), ln)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading YOLO weights...\n",
            "Our YOLO Layers\n",
            "254 ('conv_0', 'bn_0', 'leaky_1', 'conv_1', 'bn_1', 'leaky_2', 'conv_2', 'bn_2', 'leaky_3', 'conv_3', 'bn_3', 'leaky_4', 'shortcut_4', 'conv_5', 'bn_5', 'leaky_6', 'conv_6', 'bn_6', 'leaky_7', 'conv_7', 'bn_7', 'leaky_8', 'shortcut_8', 'conv_9', 'bn_9', 'leaky_10', 'conv_10', 'bn_10', 'leaky_11', 'shortcut_11', 'conv_12', 'bn_12', 'leaky_13', 'conv_13', 'bn_13', 'leaky_14', 'conv_14', 'bn_14', 'leaky_15', 'shortcut_15', 'conv_16', 'bn_16', 'leaky_17', 'conv_17', 'bn_17', 'leaky_18', 'shortcut_18', 'conv_19', 'bn_19', 'leaky_20', 'conv_20', 'bn_20', 'leaky_21', 'shortcut_21', 'conv_22', 'bn_22', 'leaky_23', 'conv_23', 'bn_23', 'leaky_24', 'shortcut_24', 'conv_25', 'bn_25', 'leaky_26', 'conv_26', 'bn_26', 'leaky_27', 'shortcut_27', 'conv_28', 'bn_28', 'leaky_29', 'conv_29', 'bn_29', 'leaky_30', 'shortcut_30', 'conv_31', 'bn_31', 'leaky_32', 'conv_32', 'bn_32', 'leaky_33', 'shortcut_33', 'conv_34', 'bn_34', 'leaky_35', 'conv_35', 'bn_35', 'leaky_36', 'shortcut_36', 'conv_37', 'bn_37', 'leaky_38', 'conv_38', 'bn_38', 'leaky_39', 'conv_39', 'bn_39', 'leaky_40', 'shortcut_40', 'conv_41', 'bn_41', 'leaky_42', 'conv_42', 'bn_42', 'leaky_43', 'shortcut_43', 'conv_44', 'bn_44', 'leaky_45', 'conv_45', 'bn_45', 'leaky_46', 'shortcut_46', 'conv_47', 'bn_47', 'leaky_48', 'conv_48', 'bn_48', 'leaky_49', 'shortcut_49', 'conv_50', 'bn_50', 'leaky_51', 'conv_51', 'bn_51', 'leaky_52', 'shortcut_52', 'conv_53', 'bn_53', 'leaky_54', 'conv_54', 'bn_54', 'leaky_55', 'shortcut_55', 'conv_56', 'bn_56', 'leaky_57', 'conv_57', 'bn_57', 'leaky_58', 'shortcut_58', 'conv_59', 'bn_59', 'leaky_60', 'conv_60', 'bn_60', 'leaky_61', 'shortcut_61', 'conv_62', 'bn_62', 'leaky_63', 'conv_63', 'bn_63', 'leaky_64', 'conv_64', 'bn_64', 'leaky_65', 'shortcut_65', 'conv_66', 'bn_66', 'leaky_67', 'conv_67', 'bn_67', 'leaky_68', 'shortcut_68', 'conv_69', 'bn_69', 'leaky_70', 'conv_70', 'bn_70', 'leaky_71', 'shortcut_71', 'conv_72', 'bn_72', 'leaky_73', 'conv_73', 'bn_73', 'leaky_74', 'shortcut_74', 'conv_75', 'bn_75', 'leaky_76', 'conv_76', 'bn_76', 'leaky_77', 'conv_77', 'bn_77', 'leaky_78', 'conv_78', 'bn_78', 'leaky_79', 'conv_79', 'bn_79', 'leaky_80', 'conv_80', 'bn_80', 'leaky_81', 'conv_81', 'permute_82', 'yolo_82', 'identity_83', 'conv_84', 'bn_84', 'leaky_85', 'upsample_85', 'concat_86', 'conv_87', 'bn_87', 'leaky_88', 'conv_88', 'bn_88', 'leaky_89', 'conv_89', 'bn_89', 'leaky_90', 'conv_90', 'bn_90', 'leaky_91', 'conv_91', 'bn_91', 'leaky_92', 'conv_92', 'bn_92', 'leaky_93', 'conv_93', 'permute_94', 'yolo_94', 'identity_95', 'conv_96', 'bn_96', 'leaky_97', 'upsample_97', 'concat_98', 'conv_99', 'bn_99', 'leaky_100', 'conv_100', 'bn_100', 'leaky_101', 'conv_101', 'bn_101', 'leaky_102', 'conv_102', 'bn_102', 'leaky_103', 'conv_103', 'bn_103', 'leaky_104', 'conv_104', 'bn_104', 'leaky_105', 'conv_105', 'permute_106', 'yolo_106')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URLs for the YOLOv3 config and weights files\n",
        "cfg_url = 'https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3.cfg'\n",
        "weights_url = 'https://pjreddie.com/media/files/yolov3.weights'\n",
        "names_url = 'https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names'\n",
        "\n",
        "# Download yolov3.cfg\n",
        "response = requests.get(cfg_url)\n",
        "with open('yolov3.cfg', 'wb') as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "# Download yolov3.weights\n",
        "response = requests.get(weights_url)\n",
        "with open('yolov3.weights', 'wb') as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "# Download coco.names\n",
        "response = requests.get(names_url)\n",
        "with open('coco.names', 'wb') as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "print(\"YOLOv3 configuration and weights files downloaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfNciY7_cdtY",
        "outputId": "891e2220-9bf4-4a05-cb0e-304c25d271af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLOv3 configuration and weights files downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "PtC5_CRLeVV8",
        "outputId": "b40c874e-a06c-4915-8b90-668f87d73a0b"
      },
      "source": [
        "print(\"Starting Detections...\")\n",
        "# Get images located in ./images folder\n",
        "mypath = \"/content/images/\"\n",
        "file_names = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "\n",
        "# Loop through images run them through our classifer\n",
        "for file in file_names:\n",
        "    # load our input image and grab its spatial dimensions\n",
        "    image = cv2.imread(mypath+file)\n",
        "    (H, W) = image.shape[:2]\n",
        "\n",
        "    # we want only the *output* layer names that we need from YOLO\n",
        "    ln = net.getLayerNames()\n",
        "    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "    # Now we contruct our blob from our input images\n",
        "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
        "    # We set our input to our image blob\n",
        "    net.setInput(blob)\n",
        "    # Then we run a forward pass through the network\n",
        "    layerOutputs = net.forward(ln)\n",
        "\n",
        "    # we initialize our lists for our detected bounding boxes, confidences, and classes\n",
        "    boxes = []\n",
        "    confidences = []\n",
        "    IDs = []\n",
        "\n",
        "    # Loop over each of the layer outputs\n",
        "    for output in layerOutputs:\n",
        "\n",
        "        # Loop over each detection\n",
        "        for detection in output:\n",
        "            # Obtain class ID and probality of detection\n",
        "            scores = detection[5:]\n",
        "            classID = np.argmax(scores)\n",
        "            confidence = scores[classID]\n",
        "\n",
        "            # We keep only the most probably predictions\n",
        "            if confidence > 0.75:\n",
        "                # We scale the bounding box coordinates relative to the image\n",
        "                # Note: YOLO actually returns the center (x, y) of the bounding\n",
        "                # box followed by the width and height of the box\n",
        "                box = detection[0:4] * np.array([W, H, W, H])\n",
        "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
        "\n",
        "                # Get the the top and left corner of the bounding box\n",
        "                # Remember we alredy have the width and height\n",
        "                x = int(centerX - (width / 2))\n",
        "                y = int(centerY - (height / 2))\n",
        "\n",
        "                # Append our list of bounding box coordinates, confidences and class IDs\n",
        "                boxes.append([x, y, int(width), int(height)])\n",
        "                confidences.append(float(confidence))\n",
        "                IDs.append(classID)\n",
        "\n",
        "    # Now we apply non-maxima suppression to reduce overlapping bounding boxes\n",
        "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.3)\n",
        "\n",
        "    # We proceed once a detection has been found\n",
        "    if len(idxs) > 0:\n",
        "        # iterate over the indexes we are keeping\n",
        "        for i in idxs.flatten():\n",
        "            # Get the bounding box coordinates\n",
        "            (x, y) = (boxes[i][0], boxes[i][1])\n",
        "            (w, h) = (boxes[i][2], boxes[i][3])\n",
        "\n",
        "            # Draw our bounding boxes and put our class label on the image\n",
        "            color = [int(c) for c in COLORS[IDs[i]]]\n",
        "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 3)\n",
        "            text = \"{}: {:.4f}\".format(LABELS[IDs[i]], confidences[i])\n",
        "            cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "    # show the output image\n",
        "    imshow(\"YOLO Detections\", image, size = 12)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Detections...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-cb49ab41cdee>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# load our input image and grab its spatial dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmypath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# we want only the *output* layer names that we need from YOLO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import requests\n",
        "\n",
        "# Check if the necessary files are present\n",
        "required_files = ['yolov3.cfg', 'yolov3.weights', 'coco.names']\n",
        "for file in required_files:\n",
        "    if not os.path.exists(file):\n",
        "        raise FileNotFoundError(f\"{file} not found. Please download the required YOLOv3 files.\")\n",
        "\n",
        "print(\"Starting Detections...\")\n",
        "\n",
        "# Path to images folder\n",
        "mypath = \"./images/\"\n",
        "\n",
        "# Get images located in ./images folder\n",
        "file_names = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "\n",
        "# Load YOLO model\n",
        "net = cv2.dnn.readNetFromDarknet('yolov3.cfg', 'yolov3.weights')\n",
        "\n",
        "# Load COCO class labels\n",
        "with open('coco.names', 'r') as f:\n",
        "    LABELS = f.read().strip().split('\\n')\n",
        "\n",
        "# Generate colors for each class label\n",
        "np.random.seed(42)\n",
        "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype='uint8')\n",
        "\n",
        "# Loop through images and run them through our classifier\n",
        "for file in file_names:\n",
        "    # Load our input image\n",
        "    image_path = join(mypath, file)\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    if image is None:\n",
        "        print(f\"Failed to load image {image_path}\")\n",
        "        continue\n",
        "\n",
        "    # Grab its spatial dimensions\n",
        "    (H, W) = image.shape[:2]\n",
        "\n",
        "    # Get the output layer names that we need from YOLO\n",
        "    ln = net.getLayerNames()\n",
        "    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "    # Construct our blob from our input images\n",
        "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
        "    net.setInput(blob)\n",
        "\n",
        "    # Run a forward pass through the network\n",
        "    layerOutputs = net.forward(ln)\n",
        "\n",
        "    # Initialize our lists for detected bounding boxes, confidences, and class IDs\n",
        "    boxes = []\n",
        "    confidences = []\n",
        "    IDs = []\n",
        "\n",
        "    # Loop over each of the layer outputs\n",
        "    for output in layerOutputs:\n",
        "        # Loop over each detection\n",
        "        for detection in output:\n",
        "            # Obtain class ID and probability of detection\n",
        "            scores = detection[5:]\n",
        "            classID = np.argmax(scores)\n",
        "            confidence = scores[classID]\n",
        "\n",
        "            # Keep only the most probable predictions\n",
        "            if confidence > 0.75:\n",
        "                # Scale the bounding box coordinates relative to the image\n",
        "                box = detection[0:4] * np.array([W, H, W, H])\n",
        "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
        "\n",
        "                # Get the top and left corner of the bounding box\n",
        "                x = int(centerX - (width / 2))\n",
        "                y = int(centerY - (height / 2))\n",
        "\n",
        "                # Append our list of bounding box coordinates, confidences, and class IDs\n",
        "                boxes.append([x, y, int(width), int(height)])\n",
        "                confidences.append(float(confidence))\n",
        "                IDs.append(classID)\n",
        "\n",
        "    # Apply non-maxima suppression to reduce overlapping bounding boxes\n",
        "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.3)\n",
        "\n",
        "    # Proceed once a detection has been found\n",
        "    if len(idxs) > 0:\n",
        "        # Iterate over the indexes we are keeping\n",
        "        for i in idxs.flatten():\n",
        "            # Get the bounding box coordinates\n",
        "            (x, y) = (boxes[i][0], boxes[i][1])\n",
        "            (w, h) = (boxes[i][2], boxes[i][3])\n",
        "\n",
        "            # Draw our bounding boxes and put our class label on the image\n",
        "            color = [int(c) for c in COLORS[IDs[i]]]\n",
        "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 3)\n",
        "            text = \"{}: {:.4f}\".format(LABELS[IDs[i]], confidences[i])\n",
        "            cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "    # Show the output image\n",
        "    cv2.imshow(\"YOLO Detections\", image)\n",
        "    cv2.waitKey(0)\n",
        "\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "hOetJVQ3dsaI",
        "outputId": "2dda08cf-4562-483b-e32a-c0e493a4cf32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Detections...\n",
            "Failed to load image ./images/image1.jpg\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "invalid index to scalar variable.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-6917dfc6e3cd>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Get the output layer names that we need from YOLO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mln\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLayerNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mln\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetUnconnectedOutLayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Construct our blob from our input images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-6917dfc6e3cd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Get the output layer names that we need from YOLO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mln\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLayerNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mln\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetUnconnectedOutLayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Construct our blob from our input images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KwuXXUngd4ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List all files in the 'images' directory\n",
        "print(\"Files in 'images' directory:\")\n",
        "print(os.listdir('images'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT9Gp82KdLDI",
        "outputId": "04cbfb5a-34b1-422a-ec80-26d6e235f2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in 'images' directory:\n",
            "['image1.jpg', 'image3.jpg', 'image2.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZjRzsrLotwA"
      },
      "source": [
        "## **NOTE:** **How to Perform non maximum suppression given boxes and corresponding scores.**\n",
        "\n",
        "```indices\t=\tcv.dnn.NMSBoxes(\tbboxes, scores, score_threshold, nms_threshold[, eta[, top_k]]```\n",
        "\n",
        "\n",
        "\n",
        "**Parameters**\n",
        "- bboxes\ta set of bounding boxes to apply NMS.\n",
        "- scores\ta set of corresponding confidences.\n",
        "- score_threshold\ta threshold used to filter boxes by score.\n",
        "- nms_threshold\ta threshold used in non maximum suppression.\n",
        "indices\tthe kept indices of bboxes after NMS.\n",
        "- eta\ta coefficient in adaptive threshold formula: nms_thresholdi+1=eta⋅nms_thresholdi.\n",
        "- top_k\tif >0, keep at most top_k picked indices.\n"
      ]
    }
  ]
}